{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVcEtGJ8QeYP"
   },
   "source": [
    "# 3. Sharpness-aware minimization (SAM) wirh Sparse Networks - 25 points\n",
    "\n",
    "## 3.1 Get a sparse networks through pruning\n",
    "**Pruning** is a technique used to reduce the size and complexity of a neural network model by removing (setting to zero) less important parameters. The goal is to create a more efficient model that retains its predictive accuracy while being smaller, which can improve both inference speed and memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilWbesreILh4"
   },
   "source": [
    "Let's train a simple model on the MNIST dataset to learn about pruning at first. We just use 10% of the dataset for both training and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TJta4OIBcEpy"
   },
   "source": [
    "### 3.1.1 Train a dense network with SGD\n",
    "Let us first train a dense model with SGD. We reuse the model for the discriminator of the GAN in Homework 2 and name it 'Classifier'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FuXSZdsCILh4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Master\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from lib.part3.utils import *\n",
    "max_epochs = 10\n",
    "device = \"cpu\" # Change this if you can and want to use a GPU device\n",
    "model = Classifier().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLI-Alb-ILh5"
   },
   "source": [
    "We define the optimizing process of SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oXZG0iPPILh5"
   },
   "outputs": [],
   "source": [
    "def optimize_sgd(model, optimizer, img, label):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(img)\n",
    "    loss = cross_entropy(output, label)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udzDwcQqX-GE"
   },
   "source": [
    "The following cell runs the training loop, this might take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "y_p6FbG0ILh5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Master\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:823: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10\\cuda\\CUDAFunctions.cpp:109.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 with 0.9325000048 accuracy on the validation set.\n",
      "Epoch 1 with 0.9508333206 accuracy on the validation set.\n",
      "Epoch 2 with 0.9566666484 accuracy on the validation set.\n",
      "Epoch 3 with 0.959166646 accuracy on the validation set.\n",
      "Epoch 4 with 0.9616666436 accuracy on the validation set.\n",
      "Epoch 5 with 0.9633333087 accuracy on the validation set.\n",
      "Epoch 6 with 0.9666666389 accuracy on the validation set.\n",
      "Epoch 7 with 0.9683333039 accuracy on the validation set.\n",
      "Epoch 8 with 0.9708333611 accuracy on the validation set.\n",
      "Epoch 9 with 0.9725000262 accuracy on the validation set.\n"
     ]
    }
   ],
   "source": [
    "train_model(model, optimizer, optimize_sgd, max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnvFT-DxILh6"
   },
   "source": [
    "#### Evaluate model\n",
    "Evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LKTkbxGyILh6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0.9773 on the test set.\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate(model)\n",
    "print(f\"Accuracy of {round(acc, 4)} on the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgVCA_jwJbPx"
   },
   "source": [
    "### 3.1.2 Sparse network with magnitude-based pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubynCgs2b0wT"
   },
   "source": [
    "Magnitude-based pruning specifically focuses on **removing weights that have the smallest absolute values**, under the assumption that weights with smaller magnitudes contribute less to the model's output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkjQPvAXcZcI"
   },
   "source": [
    "**(1)** (6 points) Realize magnitude-based pruning below, which removes a part of weights that have the smallest absolute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xfb19GShJZFI"
   },
   "outputs": [],
   "source": [
    "def magnitude_prune(model, prune_fraction):\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name and param.requires_grad:\n",
    "            # FILL: Get weight's absolute values\n",
    "            weight_abs = param.data.abs()\n",
    "            # FILL: Compute the threshold\n",
    "\n",
    "            n_total = weight_abs.numel()\n",
    "            n_prune = int(prune_fraction * n_total)\n",
    "\n",
    "            if n_prune <= 0:\n",
    "                continue\n",
    "            if n_prune >= n_total:\n",
    "                param.data.zero_()\n",
    "                continue\n",
    "\n",
    "            threshold = torch.kthvalue(weight_abs.view(-1), n_prune).values\n",
    "            # FILL: Prune weights below the threshold\n",
    "            prune_mask = weight_abs <= threshold\n",
    "            param.data[prune_mask] = 0.0\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fr0VzGC_NPtv"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "# Copy a model for pruning\n",
    "sparse_model = copy.deepcopy(model)\n",
    "# Get a sparse model by pruning 50% parameters\n",
    "sparse_model = magnitude_prune(sparse_model, prune_fraction=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sOOCdBoRgzL"
   },
   "source": [
    "Copy a sparse model for SAM implementation later in 3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Xzi_XqlmRbC4"
   },
   "outputs": [],
   "source": [
    "sparse_model_sam = copy.deepcopy(sparse_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_EFs4FiQh1X"
   },
   "source": [
    "Evaluation after pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4ZtXqwuaPtNF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0.9417 on the test set.\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate(sparse_model)\n",
    "print(f\"Accuracy of {round(acc, 4)} on the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pz5gPbMuGiAW"
   },
   "source": [
    "### 3.1.3 Finetune the sparse model\n",
    "\n",
    "We finetune the sparse model after pruning with SGD to recover its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 with 0.959166646 accuracy on the validation set.\n",
      "Epoch 1 with 0.9649999738 accuracy on the validation set.\n",
      "Epoch 2 with 0.9725000262 accuracy on the validation set.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer_sparse = torch.optim.SGD(sparse_model.parameters(), lr=0.05)\n",
    "\n",
    "finetune_epoch = 3\n",
    "train_model(sparse_model, optimizer_sparse, optimize_sgd, finetune_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jD0tZKllNi00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 with 0.9725000262 accuracy on the validation set.\n",
      "Epoch 1 with 0.9725000262 accuracy on the validation set.\n",
      "Epoch 2 with 0.9725000262 accuracy on the validation set.\n"
     ]
    }
   ],
   "source": [
    "finetune_epoch = 3\n",
    "train_model(sparse_model, optimizer, optimize_sgd, finetune_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rha2SQXBQnV8"
   },
   "source": [
    "Evaluate the sparse model after finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "sby9YSR9PrYD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0.978 on the test set.\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate(sparse_model)\n",
    "print(f\"Accuracy of {round(acc, 4)} on the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHZY8_nZaMO-"
   },
   "source": [
    "**(2)** (2 point) What are the pros and cons of sparse networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=‘blue’>\n",
    "**(2) Pros and cons of sparse networks**\n",
    "\n",
    "**Pros**\n",
    "- **Lower memory/storage**: many parameters are zero, so the model can be stored more compactly (especially with sparse formats).\n",
    "- **Potential speedups**: fewer effective multiplications can reduce compute *if* the hardware/software supports sparse ops well (or if sparsity is structured).\n",
    "- **Regularization effect**: pruning can remove weak/unused connections and sometimes improves generalization.\n",
    "- **Deployment-friendly**: can help fit models on memory-constrained devices.\n",
    "\n",
    "**Cons**\n",
    "- **Unstructured sparsity often gives little/no speedup on GPUs**: dense kernels are highly optimized; sparse kernels can have overhead and irregular memory access.\n",
    "- **Accuracy can drop** after pruning, especially with high pruning rates; usually needs finetuning.\n",
    "- **More engineering complexity**: masking, sparse formats, and tool support add complexity to training/inference pipelines.\n",
    "- **Hardware dependence**: real acceleration usually requires structured sparsity or specialized libraries/accelerators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97IxkV9Rl5bT"
   },
   "source": [
    "## 3.2 Train the sparse model with SAM\n",
    "\n",
    "Sharpness-aware minimization (SAM) is a new optimization technique, which is satisfied with not just a low loss, instead it seeks a neighborhood with uniformly low loss. SAM is motivated by the link between the geometry of the loss landscape and generalization. It makes sense that a low loss within a uniformly low loss neighborhood will generalize better than a low loss within a region of higher variance.\n",
    "\n",
    "To be specific, we consider a model with the weight vector of $\\mathbf{w}$ and the training loss $L_S$. SAM aims to minimize the maximum loss within a small region which is usually a $\\ell_2$ ball with $\\rho$ radius. Note that $\\rho$ is a small value close to $0$. Therefore, SAM can be formulated as a minimax optimization problem:\n",
    "$$\\min_{\\mathbf{w}} \\max_{\\mathbf{\\epsilon}: \\|\\mathbf{\\epsilon}\\|_2\\leq \\rho} L_S (\\mathbf{w} + \\mathbf{\\epsilon})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFY_FOCEILh3"
   },
   "source": [
    "**(3)** (3 points) Please solve the inner maximum problem by first-order Taylor expansion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2u8vP9FCtTuh"
   },
   "source": [
    "<font color=‘blue’>\n",
    "    \n",
    "**(3) Inner maximization via first-order Taylor expansion**\n",
    "\n",
    "We want to solve:\n",
    "$$\n",
    "\\max_{\\|\\epsilon\\|_2 \\le \\rho} L_S(w + \\epsilon) \n",
    "$$\n",
    "\n",
    "Using first-order Taylor expansion around &w&:\n",
    "$$L_S(w+\\epsilon) \\approx L_S(w) + \\nabla L_S(w)^\\top \\epsilon $$\n",
    "\n",
    "So the inner problem becomes:\n",
    "$$\\max_{\\|\\epsilon\\|_2 \\le \\rho} \\left(L_S(w) + \\nabla L_S(w)^\\top \\epsilon\\right) $$\n",
    "\n",
    "Since &L_S(w)& is constant w.r.t. &\\epsilon&, this is:\n",
    "$$ L_S(w) + \\max_{\\|\\epsilon\\|_2 \\le \\rho} \\nabla L_S(w)^\\top \\epsilon $$\n",
    "\n",
    "By Cauchy–Schwarz:\n",
    "$$ \\nabla L_S(w)^\\top \\epsilon \\le \\|\\nabla L_S(w)\\|_2 \\cdot \\|\\epsilon\\|_2 \\le \\rho \\|\\nabla L_S(w)\\|_2 $$\n",
    "\n",
    "The maximum is achieved when &\\epsilon& is aligned with the gradient:\n",
    "$$ \\epsilon^* = \\rho \\frac{\\nabla L_S(w)}{\\|\\nabla L_S(w)\\|_2} $$\n",
    "\n",
    "Therefore:\n",
    "$$ \\max_{\\|\\epsilon\\|_2 \\le \\rho} L_S(w + \\epsilon) \\approx L_S(w) + \\rho \\|\\nabla L_S(w)\\|_2 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwbpa28BILh6"
   },
   "source": [
    "**(4)** (8 points) Now we will train the same model using the SAM optimizer.\n",
    "Please implement SAM by the two steps below. The first step is for the maximizer which calculates $\\epsilon$ obtained in question (1). The second step is the normal step for the minimizer: $\\mathbf{w}_{t+1} = \\mathbf{w}_{t} - \\eta_t \\nabla L_S (\\mathbf{w}_t + \\mathbf{\\epsilon}_t)$ where $\\eta_t$ is step size. Note that we set $\\rho=0.05$.\n",
    "\n",
    "Hint: be careful about weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7mZeWh05ILh6"
   },
   "outputs": [],
   "source": [
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, lr=0.01, rho=0.05):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, lr)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "        self.defaults.update(self.base_optimizer.defaults)\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        # Note that p.grad gets the gradient; p.data gets the weight.\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        p.grad.norm(p=2)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        norm += 1e-12 # Avoid zero norm\n",
    "        return norm\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self):\n",
    "        # Add the perturbation on the weight.\n",
    "        # Hint: the norm of the gradient can be calculated by _grad_norm() function.\n",
    "        # Hint: self.param_groups to get access to the weight and gradient of each parameter\n",
    "        # FILL\n",
    "        grad_norm = self._grad_norm()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / grad_norm\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                e_w = p.grad * scale          # epsilon for this parameter tensor\n",
    "                p.add_(e_w)                   # w <- w + epsilon\n",
    "                self.state[p][\"e_w\"] = e_w    # save epsilon to undo later\n",
    "\n",
    "        self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        # FILL\n",
    "        # Hint: Remember to change the parameters back\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                e_w = self.state[p].get(\"e_w\", None)\n",
    "                if e_w is not None:\n",
    "                    p.sub_(e_w)               # restore: w <- w - epsilon\n",
    "\n",
    "        self.base_optimizer.step()\n",
    "\n",
    "        self.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7F6EaFSWILh6"
   },
   "source": [
    "Define an optimizer of `SAM` for the model. We recommend using `SGD` as base optimizer with a learning rate of $0.05$ (which is same with SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "f3D9IZGql74k"
   },
   "outputs": [],
   "source": [
    "base_optimizer = torch.optim.SGD\n",
    "sam_optimizer = SAM(sparse_model_sam.parameters(), base_optimizer, lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwYNhqGoILh6"
   },
   "source": [
    "**(5)** (4 points) Please define the optimizing process of SAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "q9-zRaSVCwGO"
   },
   "outputs": [],
   "source": [
    "def optimize_sam(model, optimizer, img, label):\n",
    "\n",
    "    enable_running_stats(model)\n",
    "    # First forward-backward pass\n",
    "    # FILL\n",
    "    # Hint: use sam_optimizer above\n",
    "    # Hint: use loss 'cross_entropy'\n",
    "    output = model(img)\n",
    "    loss = cross_entropy(output, label)\n",
    "    loss.backward()\n",
    "    optimizer.first_step()\n",
    "\n",
    "    disable_running_stats(model)\n",
    "    # Second forward-backward pass\n",
    "    # FILL\n",
    "    output = model(img)\n",
    "    loss = cross_entropy(output, label)\n",
    "    loss.backward()\n",
    "    optimizer.second_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "45KsL4m4DAU7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 with 0.959166646 accuracy on the validation set.\n",
      "Epoch 1 with 0.9633333087 accuracy on the validation set.\n",
      "Epoch 2 with 0.9624999762 accuracy on the validation set.\n"
     ]
    }
   ],
   "source": [
    "train_model(sparse_model_sam, sam_optimizer, optimize_sam, finetune_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhSh_GOxoNE-"
   },
   "source": [
    "#### Evaluate model\n",
    "Evaluate the sparse model finetuned with SAM on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3IAfT8aMoNWb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 0.9774 on the test set.\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate(sparse_model_sam)\n",
    "print(f\"Accuracy of {round(acc, 4)} on the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wi2NLImh_QeD"
   },
   "source": [
    "**(6)** (2 points) Give a conclusion comparing SAM with SGD. Is there any drawback of SAM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=‘blue’>\n",
    "    \n",
    "**(6) Conclusion comparing SAM with SGD + drawbacks**\n",
    "\n",
    "From the results:\n",
    "- Dense model: test accuracy $\\approx 0.977$\n",
    "- After pruning: test accuracy drops to $\\approx 0.8639$\n",
    "- Finetuning sparse with SGD: improves to $\\approx 0.9289$\n",
    "- Finetuning sparse with SAM: improves to $\\approx 0.976$, almost matching the dense model\n",
    "\n",
    "**Conclusion:** Compared to SGD, SAM gave a much larger recovery in accuracy for the pruned model. This matches the intuition that SAM prefers parameters in flatter regions by updating using the gradient at a worst-case nearby point $w+\\epsilon^*$, which often improves generalization—especially when pruning makes the landscape sharper/more fragile.\n",
    "\n",
    "**Drawbacks of SAM:**\n",
    "- **Higher training cost**: SAM requires **two forward+backward passes per step**, so training is roughly ~2× slower than SGD.\n",
    "- **Extra hyperparameter**: the neighborhood size &\\rho& needs tuning and interacts with learning rate/batch size.\n",
    "- **Care with BatchNorm**: SAM often needs special handling of running stats (enable/disable) to be stable and effective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
